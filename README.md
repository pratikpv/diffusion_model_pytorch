# Diffusion model trained on MNIST dataset using pytorch

This is a basic implementation of Diffusion model [2, 3]. 
Diffusion Models are generative models. They can generate synthetic data similar to the data on which they are trained. Diffusion Models are trained through the successive addition of Gaussian noise, and then learning to recover the data by reversing this noising process. After training, we can use the Diffusion Model to generate data by simply passing randomly sampled noise through the learned denoising process.


![ALT TEXT](https://hojonathanho.github.io/diffusion/assets/img/pgm_diagram_xarrow.png)

In this tutorial we train the Diffusion model on MNIST handwritten digits dataset. A simple u-NET model is used to generate the samples in denoising stage.

# Sample images generated by the model
 
Below images are sampled after training of 150 epochs. During the training, we gradually add Gaussian noise using linear $\beta$ schedule. To visually see the learning of denoising we sample 10 images with similar $\beta$ schedule. It can be seen that the model is able to generate hand-writen digits like MNIST data (left most samples).
The generated images are not super-clear but can be improved with more fine-tuning.

![ALT TEXT](https://github.com/pratikpv/diffusion_model_pytorch/blob/main/sample_output_images/epoch_171_batch_0.png)
![ALT TEXT](https://github.com/pratikpv/diffusion_model_pytorch/blob/main/sample_output_images/epoch_253_batch_0.png)
![ALT TEXT](https://github.com/pratikpv/diffusion_model_pytorch/blob/main/sample_output_images/epoch_289_batch_0.png)


# References
1. https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing
2. DDPM paper: https://arxiv.org/pdf/2006.11239.pdf
3. https://hojonathanho.github.io/diffusion/
4. https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/
